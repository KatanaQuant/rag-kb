# Release v0.5.0-alpha: HybridChunker & Advanced PDF Processing

**Release Date**: 2025-11-17
**Type**: Feature Release
**Status**: Alpha (Pre-release)

---

## Overview

This release implements HybridChunker via Docling 2.61.2, significantly improving document structure preservation and retrieval quality. The system combines structure-aware splitting with token-based optimization, intelligently splitting documents along natural boundaries while maximizing embedding model capacity utilization.

**Key Improvements:**
- HybridChunker with token-aware semantic chunking
- Docling 2.61.2 upgrade with advanced PDF processing
- RapidOCR support for scanned PDFs and images
- Table structure extraction and preservation
- CPU-optimized Docker build for broader compatibility
- Hot-mount development workflow

---

## Major Features

### HybridChunker with Token-Aware Semantic Chunking

**What's New:**
- Document-aware chunking that preserves semantic boundaries
- Token limit enforcement (default: 512 tokens) for optimal embeddings
- Automatic merging of small chunks for better context
- Intelligent splitting of oversized elements
- Table content kept intact within chunks
- Replaces fixed-size chunking for superior context preservation

**Performance (tested with 272-page technical book):**
- 372 semantic chunks from 482,179 characters
- Chunk sizes: 90-2,346 chars (avg: 1,296 chars / ~324 tokens)
- Better token utilization: 324 avg tokens vs 79 with fixed-size chunking
- Preserves complete concepts, code blocks, and semantic context
- Processing speed: ~0.23 pages/sec with OCR

**Configuration:**
```bash
SEMANTIC_CHUNKING=true        # Enable HybridChunker (default)
CHUNK_MAX_TOKENS=512          # Maximum tokens per chunk
USE_DOCLING=true              # Required for semantic chunking
```

**How it works:**
HybridChunker combines structure-aware splitting with token-based optimization:
1. **Token-aware chunking**: Enforces max_tokens limit to match embedding model capacity
2. **Optimal retrieval**: Fills chunks closer to token limit â†’ better embedding quality
3. **Merge small chunks**: Combines undersized chunks with neighbors
4. **Split oversized elements**: Intelligently splits large sections
5. **Better storage efficiency**: ~40% fewer chunks with higher quality

### OCR Support for Scanned PDFs

**What's New:**
- Tesseract OCR integration for scanned PDFs and images
- Automatic text extraction from images embedded in PDFs
- CPU-based OCR processing (no GPU required)
- Model caching for improved performance

**Performance:**
- Processing speed: ~0.23 pages/sec with full OCR and table extraction
- First run downloads OCR models (~500MB, cached afterward)
- Memory usage: ~1.5-2GB during OCR processing

**Configuration:**
```bash
USE_DOCLING=true              # OCR enabled by default
```

### Table Structure Extraction

**What's New:**
- Advanced table detection and structure preservation
- Table content extracted with row/column relationships intact
- Tables rendered in markdown format for readability
- Multi-column layouts properly handled

**Example:**
Tables from technical books and scientific papers are now extracted with structure preserved, making data tables, comparison matrices, and technical specifications searchable and readable.

### CPU-Optimized Build

**What's New:**
- Docker image optimized for CPU-only processing
- No GPU dependencies or CUDA requirements
- Runs on any x86_64 system with Docker
- Smaller image size (~1.5GB vs 3GB+ for GPU builds)

**Benefits:**
- Broader compatibility (laptops, servers, cloud instances)
- Lower resource requirements
- Simplified deployment

### Enhanced Processing Visibility

**What's New:**
- Stage-by-stage completion logging
- Character and chunk count reporting
- Processing time visibility
- Progress tracking for large document sets

**Example Output:**
```
Extraction complete: Practical Object-Oriented Design in Ruby.pdf - 482,179 chars extracted
Chunking complete: Practical Object-Oriented Design in Ruby.pdf - 372 semantic chunks created
Embedding complete: Practical Object-Oriented Design in Ruby.pdf - 372 chunks embedded
Indexed Practical Object-Oriented Design in Ruby.pdf: 372 chunks stored
```

### Resumable Processing

**What's New:**
- Automatic progress checkpointing
- Resume from interruption point after crashes/restarts
- Configurable batch size and retry limits
- Progress record cleanup options

**Configuration:**
```bash
RESUMABLE_PROCESSING=true     # Enable resumable processing (default)
PROCESSING_BATCH_SIZE=50      # Chunks per checkpoint
PROCESSING_MAX_RETRIES=3      # Max retries for failed files
```

---

## Installation

### New Installation

```bash
git clone https://github.com/KatanaQuant/rag-kb.git
cd rag-kb
git checkout v0.5.0-alpha

# Start with semantic chunking (default)
docker-compose up -d
```

### Upgrading from v0.4.0-alpha

```bash
cd rag-kb
git pull origin main
git checkout v0.5.0-alpha

# Backup current database
cp data/rag.db data/rag.db.v0.4.0-backup

# Rebuild with new chunking
docker-compose down
docker-compose up --build -d

# Monitor indexing
docker-compose logs -f rag-api
```

**Important:** Semantic chunking requires re-indexing. Existing databases will continue to work but won't benefit from semantic chunking until re-indexed.

---

## Breaking Changes

**None**. Fully backward compatible with v0.4.0-alpha.

- Existing databases continue to work
- PyPDF fallback available via `USE_DOCLING=false`
- Fixed-size chunking fallback if `SEMANTIC_CHUNKING=false`
- No configuration changes required (sensible defaults)

---

## Performance Considerations

### Processing Speed
- **HybridChunker + OCR + Table extraction**: ~0.23 pages/sec (272-page book in ~20 min)
- **HybridChunker without OCR**: ~0.5-1 pages/sec (estimated)
- **Fixed-size chunking**: ~10-20 pages/sec

### Memory Usage
- **With OCR**: ~1.5-2GB
- **Without OCR**: ~800MB-1.2GB

### Quality vs Speed
- Semantic chunking: Higher quality, slower processing
- Fixed-size chunking: Faster processing, lower quality
- OCR: Essential for scanned PDFs, adds processing time

**Recommendation:** Use semantic chunking + OCR for production knowledge bases. Disable OCR for digital-native PDFs to speed up processing.

---

## Migration Guide

### From v0.4.0-alpha

1. Backup your database:
   ```bash
   cp data/rag.db data/rag.db.v0.4.0-backup
   ```

2. Upgrade to v0.5.0-alpha:
   ```bash
   git pull origin main
   git checkout v0.5.0-alpha
   docker-compose down
   docker-compose up --build -d
   ```

3. Re-index your knowledge base:
   ```bash
   # Database will be automatically rebuilt
   # Monitor progress: docker-compose logs -f rag-api
   ```

4. Verify improved quality:
   ```bash
   curl -X POST http://localhost:8000/query \
     -H "Content-Type: application/json" \
     -d '{"text": "your test query", "top_k": 5}'
   ```

### Rollback to v0.4.0-alpha

If needed:

```bash
docker-compose down
cp data/rag.db.v0.4.0-backup data/rag.db
git checkout v0.4.0-alpha
docker-compose up -d
```

---

## Full Changelog

### Features

- HybridChunker with token-aware semantic chunking (docling-core 2.8.0+)
- Docling 2.61.2 upgrade with advanced PDF processing
- RapidOCR support for scanned PDFs and images
- Table structure extraction and preservation
- CPU-optimized Docker build
- Enhanced processing visibility and logging
- Resumable processing with progress checkpointing
- File watcher improvements for auto-sync
- Hot-mount development workflow

### Enhancements

- Configurable semantic chunking parameters
- Model caching for Docling, OCR, and embeddings
- Improved error handling and retry logic
- Better progress tracking for large document sets
- Database schema auto-migration
- Memory usage optimizations

### Documentation

- Updated README with semantic chunking guide
- Added configuration examples in .env.example
- Enhanced API documentation
- Added release notes and migration guide

### Infrastructure

- Docker multi-stage build optimizations
- Separate cache volumes for models
- Health check improvements
- Better resource limit defaults

### Bug Fixes

- Fixed HybridChunker initialization (requires HuggingFaceTokenizer wrapper)
- Fixed docling 2.x API compatibility (DocumentConversionInput removed)
- Fixed RapidOCR model download permissions for non-root user
- Added missing system dependencies (libgl1, libglib2.0-0)
- Improved database connection handling
- Better error messages for missing dependencies

---

## Known Issues

### First Run Download Time

Docling and OCR models (~500MB total) download on first startup. This is normal and only happens once.

**Workaround:** Pre-populate cache volumes or disable OCR for initial testing:
```bash
echo "USE_DOCLING=false" > .env
docker-compose up -d
```

### Memory Usage

OCR processing requires ~1.5-2GB RAM. On resource-constrained systems:
- Disable OCR: `USE_DOCLING=false`
- Use fixed-size chunking: `SEMANTIC_CHUNKING=false`
- Process fewer documents at once
- Increase Docker memory limits in docker-compose.yml

### Processing Time

Semantic chunking + OCR is slower than fixed-size chunking. For large knowledge bases (100+ PDFs), initial indexing may take hours. This is expected and only happens once.

**Workaround:** Process in batches or disable OCR for digital-native PDFs.

---

## Technical Details

### Chunking Algorithm

HybridChunker from docling-core 2.8.0+ combines structure-aware splitting with token-based optimization:

1. Parse document into hierarchical elements (sections, paragraphs, tables)
2. Group elements into chunks respecting semantic boundaries
3. Enforce token limits (default: 512 tokens) via HuggingFaceTokenizer
4. Merge small chunks with neighbors for better context
5. Intelligently split oversized elements
6. Preserve complete tables and code blocks

### OCR Pipeline

1. Docling 2.61.2 DocumentConverter loads PDF
2. RapidOCR (PyTorch backend) extracts text from images/scanned pages
3. Tesseract provides fallback OCR support
4. Results merged with native PDF text
5. Structure analysis via Docling's document model
6. Table detection and structure preservation

### Model Caching

Three separate cache volumes:
- `.cache/docling/` - Docling models and artifacts
- `.cache/huggingface/` - Embedding models (Arctic Embed, etc.)
- RapidOCR models cached in `/usr/local/lib/python3.11/site-packages/rapidocr/models`

Caches persist between container restarts.

---

## Testing

### Unit Tests

```bash
cd api
python -m pytest tests/ -v
```

### Integration Test

```bash
# Query with semantic chunks
curl -X POST http://localhost:8000/query \
  -H "Content-Type: application/json" \
  -d '{"text": "SOLID principles", "top_k": 5}' | jq

# Check health and chunk count
curl http://localhost:8000/health | jq
```

### Quality Verification

Compare relevance scores before and after upgrade:
- Direct matches: 60-70% (semantic) vs 50-60% (fixed)
- Related content: 55-65% (semantic) vs 45-55% (fixed)
- Context preservation: Significantly better with semantic chunking

---

## Contributors

Project maintained by KatanaQuant.

Special thanks to:
- Docling team for document understanding library
- Tesseract OCR project
- Sentence-transformers and Snowflake for embedding models

---

## Support

- **Documentation**: [README.md](https://github.com/KatanaQuant/rag-kb/blob/main/README.md)
- **Issues**: [GitHub Issues](https://github.com/KatanaQuant/rag-kb/issues)
- **Email**: horoshi@katanaquant.com

---

## What's Next

Planned for v0.6.0:

- GPU acceleration for embeddings
- Multi-model embedding support (select best model per use case)
- Advanced query rewriting and expansion
- Hybrid search improvements
- Performance optimizations for large knowledge bases

---

**Previous Release**: [v0.4.0-alpha](https://github.com/KatanaQuant/rag-kb/releases/tag/v0.4.0-alpha)
**Repository**: [https://github.com/KatanaQuant/rag-kb](https://github.com/KatanaQuant/rag-kb)
