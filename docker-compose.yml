version: '3.8'

services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    ports:
      - "11434:11434"
    volumes:
      - ./.cache/ollama:/root/.ollama  # Persist models
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 4G

  rag-api:
    build:
      context: ./api
      dockerfile: Dockerfile
      # Enable BuildKit for faster builds (requires DOCKER_BUILDKIT=1)
      cache_from:
        - rag-api:latest
    image: rag-api:latest  # Tag the built image for cache_from to work
    container_name: rag-api
    ports:
      - "0.0.0.0:${RAG_PORT:-8000}:8000"  # Listen on all interfaces for network access
    volumes:
      - ./kb:/app/kb
      - ./data:/app/data
      - ./tests:/app/tests  # DEBUGGING: Mount tests for test execution
      - ./api:/app  # DEBUGGING: Hot reload for debugging
      - ./api/migrations:/app/migrations  # Migration scripts
      - ./config:/app/yara_config  # Mount config directory for YARA rules
      - ./.cache/deepsearch_glm:/app/.cache/deepsearch_glm  # DeepSearch GLM cache
      - ./.cache/docling:/home/appuser/.cache/docling  # Docling model cache
      - ./.cache/huggingface:/home/appuser/.cache/huggingface  # Hugging Face model cache (Docling models)
      - ./.cache/easyocr:/home/appuser/.EasyOCR  # EasyOCR model cache (detection models)
      - ./.cache/rapidocr:/opt/venv/lib/python3.13/site-packages/rapidocr/models  # RapidOCR model cache (OCR models)
      - ./.cache/clamav:/var/lib/clamav  # ClamAV virus database cache (persists ~230MB signatures)
      - ./.cache/query_expansion:/app/data/query_expansion  # Query expansion cache
    environment:
      - PYTHONUNBUFFERED=1
      - MODEL_NAME=${MODEL_NAME:-Snowflake/snowflake-arctic-embed-l-v2.0}
      - BATCH_SIZE=${BATCH_SIZE:-5}
      - BATCH_DELAY=${BATCH_DELAY:-0.5}
      - USE_DOCLING=${USE_DOCLING:-true}  # Enable Docling PDF extraction (default)
      - SEMANTIC_CHUNKING=${SEMANTIC_CHUNKING:-true}  # Enable semantic chunking (default)
      - CHUNK_MAX_TOKENS=${CHUNK_MAX_TOKENS:-512}  # Max tokens per semantic chunk
      - AUTO_REPAIR_ORPHANS=${AUTO_REPAIR_ORPHANS:-true}  # Auto-repair orphaned files on startup
      - CHUNK_WORKERS=${CHUNK_WORKERS:-1}  # Concurrent chunking threads (1 for Balanced profile)
      - EMBEDDING_WORKERS=${EMBEDDING_WORKERS:-2}  # Concurrent embedding threads (2 optimal for GIL)
      - EMBEDDING_BATCH_SIZE=${EMBEDDING_BATCH_SIZE:-32}  # Chunks per batch for embedding (32 optimal for CPU)
      - MAX_PENDING_EMBEDDINGS=${MAX_PENDING_EMBEDDINGS:-6}  # Max queued embeddings before throttling
      - OMP_NUM_THREADS=${OMP_NUM_THREADS:-2}  # OpenMP threads per worker
      - MKL_NUM_THREADS=${MKL_NUM_THREADS:-2}  # MKL threads
      - OPENBLAS_NUM_THREADS=${OMP_NUM_THREADS:-2}  # OpenBLAS threads (follows OMP)
      - NUMEXPR_NUM_THREADS=${OMP_NUM_THREADS:-2}  # NumExpr threads (follows OMP)
      # Reranking (⚠️ REQUIRES GPU - disabled by default for CPU builds)
      - RERANKING_ENABLED=${RERANKING_ENABLED:-false}  # Enable only with GPU (100x slower on CPU)
      # LLM Query Expansion via Ollama
      - OLLAMA_URL=${OLLAMA_URL:-http://ollama:11434}
      - QUERY_EXPANSION_ENABLED=${QUERY_EXPANSION_ENABLED:-false}
      - QUERY_EXPANSION_MODEL=${QUERY_EXPANSION_MODEL:-qwen2.5:0.5b}
      # Search tuning (for permutation testing)
      - HNSW_EF_SEARCH=${HNSW_EF_SEARCH:-150}
      - HYBRID_SEARCH_ENABLED=${HYBRID_SEARCH_ENABLED:-true}
      - TITLE_BOOST_ENABLED=${TITLE_BOOST_ENABLED:-true}
    depends_on:
      - ollama
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: '${MAX_CPUS:-4.0}'
          memory: ${MAX_MEMORY:-8G}
